{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_oneshot(label,c):\n",
    "    arr=np.zeros((label[:,:,0].shape[0]*label[:,:,0].shape[1],c),np.uint8)\n",
    "    flat_label=label[:,:,0].flatten()\n",
    "    for x in range(len(flat_label)):\n",
    "        arr[x,flat_label[x]]=1\n",
    "    arr=np.reshape(arr,(label[:,:,0].shape[0],label[:,:,0].shape[1],c))\n",
    "    return arr\n",
    "\n",
    "def im_resize(img,size):\n",
    "#     x=np.minimum(img.shape[0],img.shape[1])\n",
    "#     re_img=img[0:x,0:x]\n",
    "    im=cv2.resize(img, (size,size), interpolation=cv2.INTER_LINEAR)\n",
    "    return im\n",
    "def label_resize(img,size):\n",
    "#     x=np.minimum(img.shape[0],img.shape[1])\n",
    "#     re_img=img[0:x,0:x]\n",
    "    im=cv2.resize(img, (size,size), interpolation=cv2.INTER_NEAREST)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_resize(img,size):\n",
    "#     x=np.minimum(img.shape[0],img.shape[1])\n",
    "#     re_img=img[0:x,0:x]\n",
    "    im=np.reshape(cv2.resize(img, (size,size), interpolation=cv2.INTER_LINEAR),(size,size,1))\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_resize(img,size):\n",
    "#     x=np.minimum(img.shape[0],img.shape[1])\n",
    "#     re_img=img[0:x,0:x]\n",
    "    im=cv2.resize(img, (size,size), interpolation=cv2.INTER_LINEAR)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_oneshot_14(label):\n",
    "    arr=np.zeros((label.shape[0]*label.shape[1],14),np.uint8)\n",
    "    flat_label=label.flatten()\n",
    "    for x in range(len(flat_label)):\n",
    "        arr[x,flat_label[x]]=1\n",
    "    arr=np.reshape(arr,(label.shape[0],label.shape[1],14))\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_relu(layer,is_use):\n",
    "    mean,var=tf.nn.moments(layer,[1,2])\n",
    "    num=tf.to_int32(mean.shape[1])\n",
    "    mean_1=tf.reshape(mean,[-1,1,1,num])\n",
    "    var_1=tf.reshape(var,[-1,1,1,num])\n",
    "    layer_1=(layer-mean_1)/var_1\n",
    "    return tf.nn.relu(layer_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_elu(layer, use_bn):\n",
    "    mean,var=tf.nn.moments(layer,[1,2])\n",
    "    exp_mean=tf.expand_dims(mean,1)\n",
    "    exp_var=tf.expand_dims(var,1)\n",
    "    exp_mean_1=tf.expand_dims(exp_mean,2)\n",
    "    exp_var_1=tf.expand_dims(exp_var,2)\n",
    "    layer_1=tf.nn.elu(layer-exp_mean_1)\n",
    "    return layer_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act_func_c(layer,use_bn):\n",
    "    layer_1=tf.nn.relu(layer)\n",
    "    return layer_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act_func_d(layer,use_bn):\n",
    "    layer_1=tf.maximum(layer,layer*0.3)\n",
    "    return layer_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_relu(layer):  \n",
    "    mean,var=tf.nn.moments(layer,[1])\n",
    "    mean_1=tf.reshape(mean,[-1,1])\n",
    "    var_1=tf.reshape(var,[-1,1])\n",
    "    layer_1=(layer-mean_1)/var_1\n",
    "    return tf.nn.relu(layer_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_wei(name,ker_s,in_l_num,out_l_num):\n",
    "    w= tf.get_variable('w_'+name, shape=[ker_s,ker_s,in_l_num,out_l_num], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    b=tf.get_variable('b_'+name, shape=[out_l_num], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_relu(layer,is_use):\n",
    "    mean,var=tf.nn.moments(layer,[1,2])\n",
    "    num=tf.to_int32(mean.shape[1])\n",
    "    mean_1=tf.reshape(mean,[-1,1,1,num])\n",
    "    var_1=tf.reshape(var,[-1,1,1,num])\n",
    "    layer_1=(layer-mean_1)/var_1\n",
    "    return tf.concat([tf.nn.relu(layer_1),tf.nn.relu(-1.0*layer_1)],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_relu_sig(layer,is_use):\n",
    "    return tf.concat([tf.nn.relu(layer),tf.nn.sigmoid(layer)],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv(in_layer,k_s,in_n,out_n,l_n,name,use_bn):\n",
    "    in_l=in_layer\n",
    "    for i in range(l_n):\n",
    "        if i==0:\n",
    "            w,b=make_wei(name+'_'+str(i+1),k_s,in_n,out_n)\n",
    "            in_l=conv_relu(tf.nn.conv2d(in_l,w, strides=[1,1,1,1], padding= 'SAME')+b,use_bn)\n",
    "        else:\n",
    "            w,b=make_wei(name+'_'+str(i+1),k_s,out_n*2,out_n)\n",
    "            in_l=conv_relu(tf.nn.conv2d(in_l,w, strides=[1,1,1,1], padding= 'SAME')+b,use_bn)\n",
    "    return in_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2_conv(in_layer,k_s,in_n,out_n,l_n,name,use_bn):\n",
    "    in_l=in_layer\n",
    "    for i in range(l_n):\n",
    "        if i==0:\n",
    "            w,b=make_wei(name+'_'+str(i+1),k_s,in_n,out_n)\n",
    "            in_l=conv_relu_sig(tf.nn.conv2d(in_l,w, strides=[1,1,1,1], padding= 'SAME')+b,use_bn)\n",
    "        else:\n",
    "            w,b=make_wei(name+'_'+str(i+1),k_s,out_n*2,out_n)\n",
    "            in_l=conv_relu_sig(tf.nn.conv2d(in_l,w, strides=[1,1,1,1], padding= 'SAME')+b,use_bn)\n",
    "    return in_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_c(in_layer,k_s,in_n,out_n,l_n,name,use_bn):\n",
    "    in_l=in_layer\n",
    "    for i in range(l_n):\n",
    "        if i==0:\n",
    "            w,b=make_wei(name+'_'+str(i+1),k_s,in_n,out_n)\n",
    "            in_l=norm_relu(tf.nn.conv2d(in_l,w, strides=[1,1,1,1], padding= 'SAME')+b,use_bn)\n",
    "        else:\n",
    "            w,b=make_wei(name+'_'+str(i+1),k_s,out_n*2,out_n)\n",
    "            in_l=norm_relu(tf.nn.conv2d(in_l,w, strides=[1,1,1,1], padding= 'SAME')+b,use_bn)\n",
    "    return in_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fc(in_layer,k_s,in_n,out_n,l_n,name,use_bn):\n",
    "    in_l=in_layer\n",
    "    for i in range(l_n):\n",
    "        if i==0:\n",
    "            a=in_n\n",
    "        else:\n",
    "            a=out_n\n",
    "        w,b=make_wei(name+'_'+str(i+1),k_s,a,out_n)\n",
    "        in_l=bn_relu(tf.nn.conv2d(in_l,w, strides=[1,1,1,1], padding= 'SAME')+b,use_bn)\n",
    "    return in_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edgemap(in_layer,k_s,in_n,out_n,name,use_bn):\n",
    "    w_1,b_1=make_wei(name+'_1',k_s,in_n,out_n)\n",
    "    in_1=bn_relu(tf.nn.conv2d(in_layer,w_1, strides=[1,1,1,1], padding= 'SAME')+b_1,use_bn)\n",
    "    \n",
    "    w,b=make_wei(name+'_2',k_s,out_n,out_n)\n",
    "    out_l=bn_relu(tf.nn.conv2d(in_1,w, strides=[1,1,1,1], padding= 'SAME')+b,use_bn)\n",
    "    return out_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fc(name,in_l_num,out_l_num,in_layer):\n",
    "    w= tf.get_variable('w_'+name, shape=[in_l_num,out_l_num], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b=tf.get_variable('b_'+name, shape=[out_l_num], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    return tf.matmul(in_layer,w)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg_deconv(in_l,k_s,i_s,edge,edge_n,in_n,out_n,name,is_train):\n",
    "    w_edge_1,b_edge_1=make_wei(name+'_1',k_s,in_n+edge_n,in_n)\n",
    "    w_edge_2,b_edge_2=make_wei(name+'_2',k_s,in_n,in_n)\n",
    "    w_edge_3,b_edge_3=make_wei(name+'_3',k_s,in_n,out_n)\n",
    "    \n",
    "\n",
    " \n",
    "    edge_1=tf.image.resize_images(in_l,[math.ceil(i_s),math.ceil(i_s)],method=tf.image.ResizeMethod.BILINEAR)\n",
    "    edge_2=tf.concat([edge,edge_1],3)\n",
    "    edge_3=bn_relu(tf.nn.conv2d(edge_2,w_edge_1, strides=[1,1,1,1], padding= 'SAME')+b_edge_1,is_train)\n",
    "    edge_4=bn_relu(tf.nn.conv2d(edge_3,w_edge_2, strides=[1,1,1,1], padding= 'SAME')+b_edge_2,is_train)\n",
    "    edge_5=bn_relu(tf.nn.conv2d(edge_4,w_edge_3, strides=[1,1,1,1], padding= 'SAME')+b_edge_3,is_train)\n",
    "\n",
    "\n",
    "    return edge_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dep_deconv(in_l,k_s,i_s,edge,edge_n,in_n,out_n,name,is_train):\n",
    "    w_edge_1,b_edge_1=make_wei(name+'_1',k_s,in_n+edge_n,in_n)\n",
    "    w_edge_2,b_edge_2=make_wei(name+'_2',k_s,in_n,in_n)\n",
    "    w_edge_3,b_edge_3=make_wei(name+'_3',k_s,in_n,out_n)\n",
    "    \n",
    "\n",
    " \n",
    "    edge_1=tf.image.resize_images(in_l,[math.ceil(i_s),math.ceil(i_s)],method=tf.image.ResizeMethod.BILINEAR)\n",
    "    edge_2=tf.concat([edge,edge_1],3)\n",
    "    edge_3=bn_relu(tf.nn.conv2d(edge_2,w_edge_1, strides=[1,1,1,1], padding= 'SAME')+b_edge_1,is_train)\n",
    "    edge_4=bn_relu(tf.nn.conv2d(edge_3,w_edge_2, strides=[1,1,1,1], padding= 'SAME')+b_edge_2,is_train)\n",
    "    edge_5=bn_relu(tf.nn.conv2d(edge_4,w_edge_3, strides=[1,1,1,1], padding= 'SAME')+b_edge_3,is_train)\n",
    "\n",
    "\n",
    "    return edge_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_mean(inter,union,label,ch):\n",
    "    m_iou=0\n",
    "    m_mean=0\n",
    "    none_zero=np.ones_like(union)\n",
    "    label=np.maximum(label,none_zero)\n",
    "    union=np.maximum(union,none_zero)\n",
    "    \n",
    "    s_iou=inter/union\n",
    "    s_mean=inter/label\n",
    "\n",
    "    \n",
    "    for m in range(ch):\n",
    "        m_iou+=s_iou[m]\n",
    "        m_mean+=s_mean[m]\n",
    "    print(\"mean_iou_acc : \", '{:.4f}'.format(m_iou/ch) )       \n",
    "    print(\"mean_mean_acc : \", '{:.4f}'.format(m_mean/ch))\n",
    "    return s_iou,s_mean,m_iou/ch,m_mean/ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(output,label,c):\n",
    "    inter=output*label\n",
    "    union_1=output+label\n",
    "    union_2=tf.ones_like(union_1)\n",
    "    union=tf.minimum(union_1,union_2)\n",
    "    \n",
    "    union_sum=tf.reduce_sum(union,0)\n",
    "    label_sum=tf.reduce_sum(label,0)\n",
    "    inter_sum=tf.reduce_sum(inter,0)\n",
    "\n",
    "    return inter_sum, label_sum,union_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dep_deri(dep_img,d_de):\n",
    "    d_x_deri = tf.reshape(d_de, [3, 3, 1, 1])\n",
    "    d_y_deri = tf.transpose(d_x_deri, [1, 0, 2, 3])\n",
    "    \n",
    "\n",
    "    d_x_deri=tf.nn.conv2d(dep_img, d_x_deri, strides=[1, 1, 1, 1], padding='VALID')\n",
    "    d_y_deri=tf.nn.conv2d(dep_img, d_y_deri, strides=[1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "    \n",
    "    d_x_loss=tf.square(d_x_deri/2.0)\n",
    "    d_y_loss=tf.square(d_y_deri/2.0)\n",
    "    \n",
    "        \n",
    "    dep_x_cost=tf.reshape(d_x_loss,[-1,int(d_x_loss.shape[1])*int(d_x_loss.shape[2])])\n",
    "    dep_y_cost=tf.reshape(d_y_loss,[-1,int(d_y_loss.shape[1])*int(d_y_loss.shape[2])])\n",
    "    dep_deri_cost=tf.reduce_mean(tf.reduce_mean(dep_x_cost,1)+tf.reduce_mean(dep_y_cost,1))\n",
    "    \n",
    "    return dep_deri_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crf(img,seg_img,dep_img,deri,num):\n",
    "    s_img=tf.square(tf.nn.atrous_conv2d(seg_img, deri,rate=num, padding='VALID'))\n",
    "    \n",
    "    im=tf.exp((-1*0.05*tf.square(tf.nn.atrous_conv2d(img, deri,rate=num, padding='VALID')))-(0.01*(num-1)))\n",
    "    \n",
    "    s_i_crf=tf.multiply(s_img,im)\n",
    "    s_d_crf=tf.multiply((1-s_img),(1-im))\n",
    "    s_sum=(tf.reduce_mean(tf.reduce_sum(s_i_crf,1))*0.5)+(tf.reduce_mean(tf.reduce_sum(s_d_crf,1))*0.5)\n",
    "    \n",
    "    d_img=tf.exp((-1*0.05*tf.square(tf.nn.atrous_conv2d(dep_img, deri,rate=num, padding='VALID')))-(0.01*(num-1)))\n",
    "    \n",
    "    d_i_crf=tf.multiply(s_img,d_img)\n",
    "    d_d_crf=tf.multiply((1-s_img),(1-d_img))\n",
    "    d_sum=(tf.reduce_mean(tf.reduce_sum(d_i_crf,1))*0.5)+(tf.reduce_mean(tf.reduce_sum(d_d_crf,1))*0.5)\n",
    "    \n",
    "    crf_sum=(s_sum*0.6)+(d_sum*0.4)\n",
    "    return crf_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg_deri(seg_img,dep_img,img,ch,s_d_1,s_d_2,s_d_3,s_d_4):\n",
    "    s_deri_1 = tf.reshape(s_d_1, [3, 3, 1, 1])\n",
    "    s_deri_2 = tf.reshape(s_d_2, [3, 3, 1, 1])\n",
    "    s_deri_3 = tf.reshape(s_d_3, [3, 3, 1, 1])\n",
    "    s_deri_4 = tf.reshape(s_d_4, [3, 3, 1, 1])\n",
    "    seg_crf=0\n",
    "    \n",
    "    for n in range(ch):\n",
    "        seg_deri=tf.slice(seg_img,[0,0,0,n],[-1,-1,-1,1])\n",
    "        for i in range(2):\n",
    "            seg_crf_1=get_crf(img,seg_deri,dep_img,s_deri_1,i+1)\n",
    "            seg_crf_2=get_crf(img,seg_deri,dep_img,s_deri_2,i+1)\n",
    "            seg_crf_3=get_crf(img,seg_deri,dep_img,s_deri_3,i+1)\n",
    "            seg_crf_4=get_crf(img,seg_deri,dep_img,s_deri_4,i+1)\n",
    "            seg_crf+=(seg_crf_1+seg_crf_2+seg_crf_3+seg_crf_4)/4.0\n",
    "    return seg_crf/ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=224\n",
    "train_n=10000\n",
    "test_n=3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label=[label_resize(cv2.imread('img_data/train_label/img-'+str(i+1).zfill(6)+'.png',0),size) for i in range(train_n)]#5285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image=[im_resize(cv2.imread('img_data/train_image/img-'+str(i+1).zfill(6)+'.jpg'),size) for i in range(train_n)]#5285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_depth=[depth_resize(cv2.imread('img_data/train_depth/img-'+str(i+1).zfill(6)+'.png',0),size) for i in range(train_n)]#5285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label=[label_resize(cv2.imread('img_data/test_label/img-'+str(i+1).zfill(6)+'.png',0),size) for i in range(test_n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image=[im_resize(cv2.imread('img_data/test_image/img-'+str(i+1).zfill(6)+'.jpg'),size) for i in range(test_n)]#5050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_depth=[depth_resize(cv2.imread('img_data/test_depth/img-'+str(i+1).zfill(6)+'.png',0),size) for i in range(test_n)]#5285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=size\n",
    "w=size\n",
    "c=14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224 224 14\n"
     ]
    }
   ],
   "source": [
    "print(h,w,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "version='18'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(train_n):\n",
    "    plt.imsave('dep_seg_'+version+'/output_img-'+str(i+1).zfill(6)+'_st.png',label_resize(cv2.imread('img_data/train_label/img-'+str(i+1).zfill(6)+'.png',0),size))\n",
    "for i in range(test_n):\n",
    "    plt.imsave('dep_seg_'+version+'/test_image/test_img-'+str(i+1).zfill(6)+'_st.png',label_resize(cv2.imread('img_data/test_label/img-'+str(i+1).zfill(6)+'.png',0),size))\n",
    "# for i in range(train_n):\n",
    "#     plt.imsave('dep_seg_'+version+'/output_img-'+str(i+1).zfill(6)+'_dt.png',d_resize(cv2.imread('img_data/train_depth/img-'+str(i+1).zfill(6)+'.png',0),size))\n",
    "# for i in range(test_n):\n",
    "#     plt.imsave('dep_seg_'+version+'/test_image/test_img-'+str(i+1).zfill(6)+'_dt.png',d_resize(cv2.imread('img_data/test_depth/img-'+str(i+1).zfill(6)+'.png',0),size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_epochs=400\n",
    "tf.reset_default_graph()\n",
    "x=tf.placeholder(tf.float32,[None,h,w,3]) \n",
    "s=tf.placeholder(tf.float32, [None,h,w])\n",
    "d=tf.placeholder(tf.float32, [None,h,w,1])\n",
    "keep_prob=tf.placeholder(tf.float32)\n",
    "train_type=tf.placeholder(tf.bool)\n",
    "global_step = tf.placeholder(tf.float32)\n",
    "seg_deri_1=tf.constant([[0,0,0],[0,1,-1],[0,0,0]], tf.float32)\n",
    "seg_deri_2=tf.constant([[0,-1,0],[0,1,0],[0,0,0]], tf.float32)\n",
    "seg_deri_3=tf.constant([[0,0,-1],[0,1,0],[0,0,0]], tf.float32)\n",
    "seg_deri_4=tf.constant([[0,0,0],[0,1,0],[0,0,-1]], tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_d_1_3=get_conv(d,3,1,16,2,'d_1',train_type)\n",
    "l_d_1_3=tf.layers.dropout(l_d_1_3,rate=keep_prob)\n",
    "l_d_1=tf.nn.max_pool(l_d_1_3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "\n",
    "l_d_2_3=get_2_conv(l_d_1,3,32,16,2,'d_2',train_type)\n",
    "l_d_2_3=tf.layers.dropout(l_d_2_3,rate=keep_prob)\n",
    "l_d_2=tf.nn.max_pool(l_d_2_3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "\n",
    "l_d_3_3=get_2_conv(l_d_2,3,32,16,3,'d_3',train_type)\n",
    "l_d_3_3=tf.layers.dropout(l_d_3_3,rate=keep_prob)\n",
    "l_d_3=tf.nn.max_pool(l_d_3_3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "\n",
    "l_d_4_3=get_2_conv(l_d_3,3,32,16,3,'d_4',train_type)\n",
    "l_d_4_3=tf.layers.dropout(l_d_4_3,rate=keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_c_1_3=get_conv(x,2,3,32,2,'c_1',train_type)\n",
    "l_c_1_2=tf.concat([l_c_1_3,l_d_1_3],3)\n",
    "l_c_1=tf.nn.max_pool(l_c_1_2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "l_c_1=tf.layers.dropout(l_c_1,rate=keep_prob)\n",
    "\n",
    "l_c_2_3=get_2_conv(l_c_1,2,64+32,128,2,'c_2',train_type)\n",
    "l_c_2_2=tf.concat([l_c_2_3,l_d_2_3],3)\n",
    "l_c_2=tf.nn.max_pool(l_c_2_2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "l_c_2=tf.layers.dropout(l_c_2,rate=keep_prob)\n",
    "\n",
    "l_c_3_3=get_2_conv(l_c_2,3,256+32,256,3,'c_3',train_type)\n",
    "l_c_3_2=tf.concat([l_c_3_3,l_d_3_3],3)\n",
    "l_c_3=tf.nn.max_pool(l_c_3_2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "l_c_3=tf.layers.dropout(l_c_3,rate=keep_prob)\n",
    "\n",
    "l_c_4_3=get_2_conv(l_c_3,3,512+32,512,3,'c_4',train_type)\n",
    "l_c_4_2=tf.concat([l_c_4_3,l_d_4_3],3)\n",
    "l_c_4=tf.nn.max_pool(l_c_4_2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "l_c_4=tf.layers.dropout(l_c_4,rate=keep_prob)\n",
    "\n",
    "l_c_5_3=get_2_conv(l_c_4,3,1024+32,512,4,'c_5',train_type)\n",
    "l_c_5=tf.layers.dropout(l_c_5_3,rate=keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_c_4_a=get_edgemap(l_c_4_2,3,1024+32,128,'c_4_a', train_type)\n",
    "l_c_4_a=tf.layers.dropout(l_c_4_a,rate=keep_prob)\n",
    "\n",
    "l_c_3_a=get_edgemap(l_c_3_2,3,512+32,128,'c_3_a', train_type)\n",
    "l_c_3_a=tf.layers.dropout(l_c_3_a,rate=keep_prob)\n",
    "\n",
    "l_c_2_a=get_edgemap(l_c_2_2,3,256+32,64,'c_2_a', train_type)\n",
    "l_c_2_a=tf.layers.dropout(l_c_2_a,rate=keep_prob)\n",
    "\n",
    "l_c_1_a=get_edgemap(l_c_1_2,3,64+32,64,'c_1_a', train_type)\n",
    "l_c_1_a=tf.layers.dropout(l_c_1_a,rate=keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_seg_5=get_2_conv(l_c_5,1,1024,512,3,'seg_5',train_type)\n",
    "l_seg_5=tf.layers.dropout(l_seg_5,rate=keep_prob)\n",
    "\n",
    "l_seg_5_s=tf.image.resize_images(l_seg_5,[math.ceil(h/8),math.ceil(h/8)],method=tf.image.ResizeMethod.BILINEAR)\n",
    "l_seg_5_s=tf.concat([l_seg_5_s,l_c_4_a],3)\n",
    "l_seg_4=get_2_conv(l_seg_5_s,3,1024+128,256,3,'seg_4',train_type)\n",
    "l_seg_4=tf.layers.dropout(l_seg_4,rate=keep_prob)\n",
    "\n",
    "l_seg_4_s=tf.image.resize_images(l_seg_4,[math.ceil(h/4),math.ceil(h/4)],method=tf.image.ResizeMethod.BILINEAR)\n",
    "l_seg_4_s=tf.concat([l_seg_4_s,l_c_3_a],3)\n",
    "l_seg_3=get_2_conv(l_seg_4_s,3,512+128,128,3,'seg_3',train_type)\n",
    "l_seg_3=tf.layers.dropout(l_seg_3,rate=keep_prob)\n",
    "\n",
    "l_seg_3_s=tf.image.resize_images(l_seg_3,[math.ceil(h/2),math.ceil(h/2)],method=tf.image.ResizeMethod.BILINEAR)\n",
    "l_seg_3_s=tf.concat([l_seg_3_s,l_c_2_a],3)\n",
    "l_seg_2=get_2_conv(l_seg_3_s,3,256+64,128,3,'seg_2',train_type)\n",
    "\n",
    "\n",
    "\n",
    "l_seg_2_s=tf.image.resize_images(l_seg_2,[math.ceil(h),math.ceil(h)],method=tf.image.ResizeMethod.BILINEAR)\n",
    "l_seg_2_s=tf.concat([l_seg_2_s,l_c_1_a],3)\n",
    "l_seg_1=get_2_conv(l_seg_2_s,3,256+64,64,3,'seg_1',train_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_seg_0=get_fc(l_seg_1,5,128, 128,3,'seg_0',train_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_seg_f_1,b_seg_f_1=make_wei('seg_f_1',3,128,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_seg_f=tf.nn.conv2d(l_seg_0,w_seg_f_1, strides=[1,1,1,1], padding= 'SAME')+b_seg_f_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_f_output=tf.reshape(l_seg_f,[-1,c])\n",
    "\n",
    "s_f_int=tf.to_int64(s)\n",
    "\n",
    "flat_s_f=tf.reshape(s_f_int,[-1,c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_one_f=tf.one_hot(s_f_int,c)\n",
    "l_seg_sig=tf.nn.sigmoid(l_seg_f)\n",
    "l_seg_avg_3=tf.nn.avg_pool(l_seg_sig, ksize=[1,3,3,1], strides=[1,1,1,1], padding='SAME')\n",
    "l_seg_last=(l_seg_sig*0.4)+(l_seg_avg_3*0.6)\n",
    "label_1=tf.nn.avg_pool(s_one_f, ksize=[1,3,3,1], strides=[1,1,1,1], padding='SAME')\n",
    "label=(s_one_f*0.4)+(label_1*0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_cost=tf.nn.sigmoid_cross_entropy_with_logits(logits=l_seg_f,labels=label)\n",
    "seg_cost=tf.reshape(seg_cost,[-1,size*size*c])\n",
    "seg_cost=tf.reduce_mean(seg_cost,1)\n",
    "seg_cost=tf.reduce_mean(seg_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_image_1=tf.argmax(l_seg_last,3)\n",
    "s_c_image=tf.nn.sigmoid(l_seg_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_one_hot=tf.one_hot(s_image_1,c)\n",
    "f_seg_o=tf.reshape(seg_one_hot,[-1,c])\n",
    "f_s_o=tf.reshape(s_one_f,[-1,c])\n",
    "\n",
    "inter,label,union=acc(f_seg_o,f_s_o,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_f_5=tf.reshape(tf.nn.max_pool(l_c_5_3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME'),[-1,7*7*1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_f_5_1 = tf.nn.relu(make_fc('f_5',7*7*1024,512,l_f_5))\n",
    "l_fc_1 = tf.nn.relu(make_fc('fc_1',512,256,l_f_5_1))\n",
    "l_fc_2 = make_fc('fc_2',256,c,l_fc_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_pro=tf.reshape(seg_one_hot,[-1,h*w,c])\n",
    "s_pro=tf.div(tf.reduce_sum(s_pro,1),(h*w))+0.0001\n",
    "fc_soft=tf.nn.softmax(l_fc_2)+0.0001\n",
    "fc_cost=tf.reduce_sum(fc_soft*tf.log(tf.div(fc_soft,s_pro)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc_cost=tf.nn.sigmoid_cross_entropy_with_logits(logits=l_fc_2,labels=s_pro)\n",
    "# fc_cost=tf.reduce_sum(fc_cost,1)\n",
    "# fc_cost=tf.reduce_mean(fc_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray=tf.image.rgb_to_grayscale(x)\n",
    "crf_cost=seg_deri(l_seg_sig,d,gray,c,seg_deri_1,seg_deri_2,seg_deri_3,seg_deri_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost=seg_cost+dep_cost+dep_seg_cost*0.1\n",
    "cost=(seg_cost*0.5)+(fc_cost*0.3)+(crf_cost*0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "starter_learning_rate = 0.0001\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,5, 0.9, staircase=True)\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning start\n",
      "epoch :  0\n",
      "cost :  17.03071 seg_cost :  0.73770\n",
      "mean_iou_acc :  0.1055\n",
      "mean_mean_acc :  0.1700\n",
      "7361.820220708847\n",
      "\n",
      "\n",
      "----------------------------test_network----------------------------\n",
      "cost :  17.87916 seg_cost :  0.88594\n",
      "mean_iou_acc :  0.0867\n",
      "mean_mean_acc :  0.1646\n",
      "397.3456230163574\n",
      "\n",
      "\n",
      "train_iou\n",
      "[ 0.258  0.123  0.003  0.011  0.1    0.404  0.06   0.015  0.005  0.023\n",
      "  0.049  0.001  0.407  0.018]\n",
      "train_mean\n",
      "[ 0.49   0.186  0.005  0.017  0.136  0.655  0.075  0.017  0.007  0.029\n",
      "  0.059  0.002  0.678  0.023]\n",
      "\n",
      "\n",
      "test_iou\n",
      "[ 0.139  0.     0.001  0.001  0.134  0.38   0.     0.     0.     0.     0.114\n",
      "  0.001  0.438  0.005]\n",
      "test_mean\n",
      "[ 0.182  0.     0.001  0.001  0.483  0.877  0.     0.     0.     0.     0.199\n",
      "  0.001  0.554  0.005]\n",
      "\n",
      "\n",
      "0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver=tf.train.Saver()\n",
    "# saver.restore(sess,\"dep_seg_\"+version+\"/seg_model.ckpt\")\n",
    "print(\"Learning start\")\n",
    "for epoch in range(training_epochs):\n",
    "    a=0\n",
    "    batch_size=2\n",
    "    total_batch =math.ceil(train_n/batch_size)\n",
    "    start=time.time()\n",
    "    avg_cost=0\n",
    "    avg_s_cost=0\n",
    "    \n",
    "    avg_i_cost=0\n",
    "\n",
    "    avg_inter=np.zeros((c))\n",
    "    avg_gt=np.zeros((c))\n",
    "    avg_union=np.zeros((c))\n",
    "\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        if a+batch_size<train_n:\n",
    "            batch_xs=train_image[a:a+batch_size]\n",
    "            batch_ys=train_label[a:a+batch_size]\n",
    "            batch_ds=train_depth[a:a+batch_size]\n",
    "\n",
    "        \n",
    "        feed_dict={x:batch_xs, s:batch_ys, d:batch_ds, keep_prob:0.5, train_type:True, global_step:epoch}\n",
    "\n",
    "        rate,in_1,gt_1,un_1,s_c,total_c,_=sess.run([learning_rate,inter,label,union,seg_cost,cost,optimizer], feed_dict=feed_dict)\n",
    "        s_c_img,s_img_1=sess.run([s_c_image, s_image_1], feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "        for j in range(len(batch_xs)):\n",
    "            plt.imsave('dep_seg_'+version+'/output_img-'+str(a+j+1).zfill(6)+'_se.png',s_img_1[j])\n",
    "    \n",
    "        for k in range(batch_size):\n",
    "            for q in range(c):\n",
    "                plt.imsave('dep_seg_'+version+'/t/class_img_'+str(a+k+1)+'_'+str(q).zfill(2)+'.png',s_c_img[k,:,:,q])\n",
    "\n",
    "        a=a+batch_size\n",
    "        avg_cost+=total_c\n",
    "        avg_s_cost+=s_c\n",
    "\n",
    "\n",
    "        avg_inter+=in_1\n",
    "        avg_gt+=gt_1\n",
    "        avg_union+=un_1\n",
    "\n",
    "\n",
    "\n",
    "    print(\"epoch : \", epoch)\n",
    "    print(\"cost : \", '{:.5f}'.format(avg_cost/total_batch),\"seg_cost : \", '{:.5f}'.format(avg_s_cost/total_batch))\n",
    "    s_iou_1,s_mean_1,m_iou_1,m_mean_1=iou_mean(avg_inter,avg_union,avg_gt,c)\n",
    "    \n",
    "    saver.save(sess, \"dep_seg_\"+version+\"/seg_model.ckpt\")\n",
    "    end=time.time()\n",
    "    print(end-start)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    \n",
    "    if epoch%2==0:\n",
    "        test_a=0\n",
    "        test_batch_size=2\n",
    "        test_total_batch = math.ceil(test_n/test_batch_size)\n",
    "        test_avg_cost=0\n",
    "        test_avg_s_cost=0\n",
    "\n",
    "        \n",
    "        test_avg_inter_1=np.zeros((c))\n",
    "        test_avg_gt_1=np.zeros((c))\n",
    "        test_avg_union_1=np.zeros((c))\n",
    "\n",
    "        t_start=time.time()\n",
    "        for i in range(test_total_batch):\n",
    "            if test_a+test_batch_size<test_n:\n",
    "                test_batch_xs=test_image[test_a:test_a+test_batch_size]\n",
    "                test_batch_ys=test_label[test_a:test_a+test_batch_size]\n",
    "                test_batch_ds=test_depth[test_a:test_a+test_batch_size]\n",
    "\n",
    "            test_feed_dict={x:test_batch_xs, s:test_batch_ys, d:test_batch_ds, keep_prob:1.0,train_type:False,global_step:epoch}\n",
    "            t_in_1,t_gt_1,t_un_1,t_s_c,t_c,t_s_img_1=sess.run([inter,label,union, seg_cost,cost,s_image_1], feed_dict=test_feed_dict)\n",
    "            for k in range(len(test_batch_xs)):\n",
    "                plt.imsave('dep_seg_'+version+'/test_image/test_img-'+str(test_a+k+1).zfill(6)+'_se_1.png',t_s_img_1[k])\n",
    "                \n",
    "            test_a=test_a+test_batch_size\n",
    "            test_avg_cost+=t_c\n",
    "            test_avg_s_cost+=t_s_c\n",
    "            \n",
    "\n",
    "            test_avg_inter_1+=t_in_1\n",
    "            test_avg_gt_1+=t_gt_1\n",
    "            test_avg_union_1+=t_un_1\n",
    "            \n",
    "        print(\"----------------------------test_network----------------------------\")\n",
    "        print(\"cost : \",'{:.5f}'.format(test_avg_cost/test_total_batch), \"seg_cost : \",'{:.5f}'.format(test_avg_s_cost/test_total_batch))\n",
    "        test_s_iou_1,test_s_mean_1,test_m_iou_1,test_m_mean_1=iou_mean(test_avg_inter_1,test_avg_union_1,test_avg_gt_1,c)\n",
    "        t_end=time.time()\n",
    "        print(t_end-t_start)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    if epoch%4==0:\n",
    "        print(\"train_iou\")\n",
    "        print(np.round(s_iou_1,3))\n",
    "        print(\"train_mean\")\n",
    "        print(np.round(s_mean_1,3))\n",
    "        print(\"\\n\")\n",
    "        print(\"test_iou\")\n",
    "        print(np.round(test_s_iou_1,3))\n",
    "        print(\"test_mean\")\n",
    "        print(np.round(test_s_mean_1,3))\n",
    "        print(\"\\n\")\n",
    "        print(rate)\n",
    "        print(\"\\n\")\n",
    "        feed_dict={x:batch_xs, s:batch_ys, d:batch_ds, keep_prob:1.0, train_type:True, global_step:epoch}\n",
    "        seg_img_1,seg_img_2,seg_img_3,seg_img_4=sess.run([l_c_1_a,l_c_2_a,l_c_3_a,l_c_4_a], feed_dict=feed_dict)\n",
    "        \n",
    "        for q in range(64):\n",
    "            plt.imsave('dep_seg_'+version+'/edge_map/edge_map_1_'+str(q).zfill(2)+'.png',seg_img_1[0,:,:,q])\n",
    "            plt.imsave('dep_seg_'+version+'/edge_map/edge_map_2_'+str(q).zfill(2)+'.png',seg_img_2[0,:,:,q])\n",
    "            plt.imsave('dep_seg_'+version+'/edge_map/edge_map_3_'+str(q).zfill(2)+'.png',seg_img_3[0,:,:,q])\n",
    "            plt.imsave('dep_seg_'+version+'/edge_map/edge_map_4_'+str(q).zfill(2)+'.png',seg_img_4[0,:,:,q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000228768\n"
     ]
    }
   ],
   "source": [
    "print(rate)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
